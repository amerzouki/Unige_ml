{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Régression logistique - Analyse de sentiment des tweets\n",
    "\n",
    "Vous allez apprendre la régression logistique. Concrètement, vous allez implémenter la régression logistique pour l'analyse de sentiment sur des tweets. À partir d'un tweet, vous déciderez s'il a un sentiment positif ou négatif. Plus précisément, vous allez :\n",
    "\n",
    "* Apprendre à extraire des caractéristiques pour la régression logistique à partir d'un texte.\n",
    "* Implémenter la régression logistique à partir de zéro.\n",
    "* Appliquer la régression logistique à une tâche de traitement du langage naturel.\n",
    "* Tester votre régression logistique.\n",
    "* Effectuer une analyse des erreurs.\n",
    "\n",
    "Nous utiliserons un ensemble de données de tweets. Espérons que vous obtiendrez une précision de plus de 99%. Exécutez la cellule ci-dessous pour charger les packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation de librairies manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/site-packages (from nltk) (2023.8.8)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/site-packages (from nltk) (4.66.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports (librairies, fonctions et données)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importe librairie nltk\n",
    "import nltk\n",
    "from os import getcwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import twitter_samples \n",
    "\n",
    "from utils import process_tweet, build_freqs # importe les fonctions d'aide à partir du fichier `utils.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préparation des données\n",
    "\n",
    "* Les données `twitter_samples` contiennent des sous-ensembles de 5 000 tweets positifs, 5 000 tweets négatifs et l'ensemble complet de 10 000 tweets.\n",
    "\n",
    "* Nous allons sélectionner uniquement les 5 000 tweets positifs et les 5 000 tweets négatifs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupère les ensembles de tweets positifs and négatifs\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions**: Vérifiez le nombre de tweets positifs et de tweets négatifs\n",
    "<details>\n",
    "<summary>Aide</summary>\n",
    "\n",
    "Utilisez la fonction `len(...)`\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "### Code ici\n",
    "print(len(all_positive_tweets))\n",
    "print(len(all_negative_tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Instructions***: Diviser les données en ensembles d'entraînement et de test, avec 20% des données dans l'ensemble de test et 80% dans l'ensemble d'entraînement.\n",
    "<details>\n",
    "<summary>Aide</summary>\n",
    "\n",
    "* Pour sélectionner les X premiers éléments d'une liste L, vous pouvez utiliser `L[:X]`.\n",
    "* Pour sélectionner les éléments restants (N-X) si la liste L est de longeur N, vous pouvez utiliser `L[X:]`.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divise les données en deux ensembles, l'un pour l'entrainement et l'autre pour le test \n",
    "### Code ici - Début\n",
    "test_pos = all_positive_tweets[4000:]\n",
    "train_pos = all_positive_tweets[:4000]\n",
    "test_neg = all_negative_tweets[4000:]\n",
    "train_neg = all_negative_tweets[:4000]\n",
    "### Code ici - Fin\n",
    "\n",
    "# Combine les tweets positifs et négatifs des deux ensembles\n",
    "train_x = train_pos + train_neg \n",
    "test_x = test_pos + test_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_x) = 8000\n",
      "len(test_x) = 2000\n"
     ]
    }
   ],
   "source": [
    "# Vérifie la taille des ensembles d'entrainment et de test\n",
    "print(\"len(train_x) = \" + str(len(train_x)))\n",
    "print(\"len(test_x) = \" + str(len(test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine les étiquettes positives et négatives\n",
    "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
    "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_y.shape = (8000, 1)\n",
      "test_y.shape = (2000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Affiche la taille des ensembles d'entrainment et de test\n",
    "print(\"train_y.shape = \" + str(train_y.shape))\n",
    "print(\"test_y.shape = \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Créez le dictionnaire de fréquence en utilisant la fonction `build_freqs()` importée.\n",
    "* Il est vivement recommandé d'ouvrir `utils.py` et de lire la fonction `build_freqs()` pour comprendre ce qu'elle fait.\n",
    "\n",
    "* `build_freqs()` est une fonction qui compte combien de fois un mot dans le 'corpus' (l'ensemble complet des tweets) était associé à une étiquette positive '1' ou à une étiquette négative '0', puis construit le dictionnaire `freqs`, où chaque clé `key` est un tuple (mot, étiquette) et la valeur `value` est le compte de sa fréquence dans le corpus des tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(freqs) = <class 'dict'>\n",
      "len(freqs) = 11337\n"
     ]
    }
   ],
   "source": [
    "# Crée le dictionnaire des férquences\n",
    "freqs = build_freqs(train_x, train_y)\n",
    "\n",
    "# Vérifie le résultat\n",
    "print(\"type(freqs) = \" + str(type(freqs)))\n",
    "print(\"len(freqs) = \" + str(len(freqs.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions**: Affichez les 20 éléments du dictionnaire `freqs` avec les plus grandes valeurs (fréquences)\n",
    "* Rappel: Chaque élément (`item`) du dictionnaire `freqs` a pour clé (`key`) une pair (mot, étiquette) et une valeur (`value`) correspondant à la fréquence. \n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>Aide</summary>\n",
    "\n",
    "#### Exemple : Trier un dictionnaire par valeur dans l'ordre décroissant \n",
    "sorted_list = sorted(my_dict.items(), key = lambda x:x[1], reverse = True)\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((':(', 0.0), 3663), ((':)', 1.0), 2847), ((':-)', 1.0), 543), (('thank', 1.0), 504), ((':d', 1.0), 498), ((':-(', 0.0), 378), (('love', 1.0), 336), (('follow', 1.0), 319), (('...', 0.0), 282), ((\"i'm\", 0.0), 266), (('follow', 0.0), 245), (('pleas', 0.0), 243), (('miss', 0.0), 239), (('...', 1.0), 227), (('♛', 0.0), 210), (('》', 0.0), 210), (('good', 1.0), 191), (('day', 1.0), 187), (('like', 1.0), 187), (('want', 0.0), 185)]\n"
     ]
    }
   ],
   "source": [
    "### Code ici\n",
    "sorted_freqs = sorted(freqs.items(), key = lambda x:x[1], reverse = True)\n",
    "print(sorted_freqs[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process tweet\n",
    "La fonction `process_tweet()` fournie tokenise le tweet en mots individuels, supprime les mots vides (stop words) et applique la racinisation (stemming)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example of a positive tweet: \n",
      " #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "\n",
      "This is an example of the processed version of the tweet: \n",
      " ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "# test de la fonction\n",
    "print('This is an example of a positive tweet: \\n', train_x[0])\n",
    "print('\\nThis is an example of the processed version of the tweet: \\n', process_tweet(train_x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected output\n",
    "```\n",
    "This is an example of a positive tweet: \n",
    " #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
    " \n",
    "This is an example of the processes version: \n",
    " ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Regression Logistique\n",
    "\n",
    "\n",
    "### Sigmoide\n",
    "$$ h(z) = \\frac{1}{1+\\exp^{-z}} \\tag{1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z): \n",
    "    '''\n",
    "    Input:\n",
    "        z: is the input (can be a scalar or an array)\n",
    "    Output:\n",
    "        h: the sigmoid of z\n",
    "    '''\n",
    "  \n",
    "    # calcule la sigmoide de z\n",
    "    h = 1/(1+np.exp(-1*z))\n",
    "    \n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Logistique: regression et sigmoide\n",
    "\n",
    "La régression logistique prend une régression linéaire régulière et applique une fonction sigmoïde à la sortie de la régression linéaire.\n",
    "\n",
    "Régression linéaire :\n",
    "$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$\n",
    "Notez que les valeurs $\\theta$ sont les \"poids\".\n",
    "\n",
    "Régression logistique :\n",
    "$$ h(z) = \\frac{1}{1+\\exp^{-z}}$$\n",
    "$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partie 1.2 Fonction de coût et Gradient\n",
    "\n",
    "La fonction de coût utilisée pour la régression logistique est la moyenne de la cross-entropie sur l'ensemble des exemples d'entraînement :\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)}))\\tag{5} $$\n",
    "* $m$ est le nombre d'exemples d'entraînement.\n",
    "* $y^{(i)}$ est l'étiquette réelle du i-ème exemple d'entraînement.\n",
    "* $h(z(\\theta)^{(i)})$ est la prédiction du modèle pour le i-ème exemple d'entraînement.\n",
    "\n",
    "La fonction de coût pour un seul exemple d'entraînement est\n",
    "$$ -1 \\times \\left( y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)})) \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mise à jour des poids\n",
    "\n",
    "Pour mettre à jour le vecteur de poids $\\theta$, vous appliquerez la descente de gradient pour améliorer itérativement les prédictions de votre modèle.\n",
    "\n",
    "Le gradient de la fonction de coût $J$ par rapport à l'un des poids $\\theta_j$ est donné par :\n",
    "\n",
    "$$\\nabla_{\\theta_j}J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m(h^{(i)}-y^{(i)})x_j \\tag{5}$$\n",
    "\n",
    "* 'i' est l'indice à travers les 'm' exemples d'entraînement.\n",
    "* 'j' est l'indice du poids $\\theta_j$, donc $x_j$ est la caractéristique associée au poids $\\theta_j$.\n",
    "\n",
    "Pour mettre à jour le poids $\\theta_j$, nous l'ajustons en soustrayant une fraction du gradient déterminée par $\\alpha$ :\n",
    "\n",
    "$$\\theta_j = \\theta_j - \\alpha \\times \\nabla_{\\theta_j}J(\\theta)$$\n",
    "\n",
    "* Le taux d'apprentissage $\\alpha$ est une valeur que nous choisissons pour contrôler la taille d'une seule mise à jour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions : Implémentez la fonction de descente de gradient\n",
    "* Le nombre d'itérations `num_iters` est le nombre de fois que vous utiliserez l'ensemble complet d'entraînement.\n",
    "* À chaque itération, vous calculerez la fonction de coût en utilisant tous les exemples d'entraînement (il y a `m` exemples d'entraînement) et pour toutes les caractéristiques.\n",
    "* Au lieu de mettre à jour un seul poids $\\theta_i$ à la fois, nous pouvons mettre à jour tous les poids dans le vecteur colonne :\n",
    "$$\\mathbf{\\theta} = \\begin{pmatrix}\n",
    "\\theta_0\n",
    "\\\\\n",
    "\\theta_1\n",
    "\\\\ \n",
    "\\theta_2 \n",
    "\\\\ \n",
    "\\vdots\n",
    "\\\\ \n",
    "\\theta_n\n",
    "\\end{pmatrix}$$\n",
    "* $\\mathbf{\\theta}$ a des dimensions (n+1, 1), où 'n' est le nombre de caractéristiques, et il y a un élément de plus pour le terme de biais $\\theta_0$ (notez que la valeur de caractéristique correspondante $\\mathbf{x_0}$ est 1).\n",
    "* Les 'logits', 'z', sont calculés en multipliant la matrice de caractéristiques 'x' par le vecteur de poids 'theta'.  $z = \\mathbf{x}\\mathbf{\\theta}$\n",
    "    * $\\mathbf{x}$ a des dimensions (m, n+1) \n",
    "    * $\\mathbf{\\theta}$ a des dimensions (n+1, 1)\n",
    "    * $\\mathbf{z}$ a des dimensions (m, 1)\n",
    "* La prédiction 'h' est calculée en appliquant la sigmoïde à chaque élément de 'z' : $h(z) = sigmoid(z)$, et a des dimensions (m,1).\n",
    "* La fonction de coût $J$ est calculée en prenant le produit scalaire des vecteurs 'y' et 'log(h)'.  Comme 'y' et 'h' sont tous deux des vecteurs colonnes (m,1), transposez le vecteur à gauche, de sorte que la multiplication matricielle d'un vecteur ligne avec un vecteur colonne effectue le produit scalaire.\n",
    "$$J = \\frac{-1}{m} \\times \\left(\\mathbf{y}^T \\cdot log(\\mathbf{h}) + \\mathbf{(1-y)}^T \\cdot log(\\mathbf{1-h}) \\right)$$\n",
    "* La mise à jour de $\\theta$ est également vectorisée. Parce que les dimensions de $\\mathbf{x}$ sont (m, n+1), et à la fois $\\mathbf{h}$ et $\\mathbf{y}$ sont (m, 1), nous devons transposer $\\mathbf{x}$ et le placer à gauche afin de réaliser la multiplication matricielle, ce qui nous donne ensuite la réponse dont nous avons besoin de dimensions (n+1, 1) :\n",
    "$$\\mathbf{\\theta} = \\mathbf{\\theta} - \\frac{\\alpha}{m} \\times \\left( \\mathbf{x}^T \\cdot \\left( \\mathbf{h-y} \\right) \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>Aide</summary>\n",
    "\n",
    "Voir la méthode np.dot pour le produit matriciel\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(x, y, theta, alpha, num_iters):\n",
    "    '''\n",
    "    Input:\n",
    "        x: matrix of features which is (m,n+1)\n",
    "        y: corresponding labels of the input matrix x, dimensions (m,1)\n",
    "        theta: weight vector of dimension (n+1,1)\n",
    "        alpha: learning rate\n",
    "        num_iters: number of iterations you want to train your model for\n",
    "    Output:\n",
    "        J: the final cost\n",
    "        theta: your final weight vector\n",
    "    Hint: you might want to print the cost to make sure that it is going down.\n",
    "    '''\n",
    "    # 'm', Nombre de ligne de la matrice x\n",
    "    m = x.shape[0]\n",
    "    \n",
    "    for i in range(0, num_iters):\n",
    "        \n",
    "        ### Code ici - Début\n",
    "        # Calculez z, produit scalaire de x et theta (combinaiason linéaire)\n",
    "        z = x.dot(theta)\n",
    "        \n",
    "        # Calculez z la sigmoide de z\n",
    "        h = sigmoid(z)\n",
    "        ### Code ici - Fin\n",
    "\n",
    "        # Calcule la fonction de coût avec les m observations\n",
    "        J = float(-1/m)*(np.transpose(y).dot(np.log(h))+np.transpose(1-y).dot(np.log(1-h)))\n",
    "        \n",
    "        # Met à jour les poids theta\n",
    "        theta = theta-alpha*np.transpose(x).dot((h-y))/m\n",
    "        \n",
    "    \n",
    "    J = float(J)\n",
    "    return J, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost after training is 0.62815242.\n",
      "The resulting vector of weights is [3.49e-06, 0.00201995, -0.00066304]\n"
     ]
    }
   ],
   "source": [
    "# Vérifiez la fonction\n",
    "# Construire un cas de test synthétique\n",
    "np.random.seed(1)\n",
    "# X de taille 10 x 3\n",
    "tmp_X = np.append(np.ones((10, 1)), np.random.rand(10, 2) * 2000, axis=1)\n",
    "# Y étiquettes de tailles 10 x 1\n",
    "tmp_Y = (np.random.rand(10, 1) > 0.35).astype(float)\n",
    "# Intialisez les poids à zéros de taille 3x1\n",
    "tmp_theta = np.zeros((3, 1))\n",
    "# Initialisez le nombre de d'itétations à 10000 et le taux d'apprentissage à 1e-8\n",
    "tmp_num_iters = 10000\n",
    "tmp_alpha = 1e-8\n",
    "# Appliquez la descente de gradient\n",
    "tmp_J, tmp_theta = gradientDescent(tmp_X, tmp_Y, tmp_theta, tmp_alpha, tmp_num_iters)\n",
    "print(f\"The cost after training is {tmp_J:.8f}.\")\n",
    "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(tmp_theta)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 2 : Extraction des caractéristiques\n",
    "\n",
    "* Étant donné une liste de tweets, extrayez les caractéristiques et stockez-les dans une matrice. Vous allez extraire deux caractéristiques.\n",
    "    * La première caractéristique est le nombre de mots positifs dans un tweet.\n",
    "    * La deuxième caractéristique est le nombre de mots négatifs dans un tweet.\n",
    "* Ensuite, entraînez votre classificateur de régression logistique sur ces caractéristiques.\n",
    "* Testez le classificateur sur un ensemble de validation.\n",
    "\n",
    "### Instructions : Implémentez la fonction extract_features.\n",
    "* Cette fonction prend en entrée un seul tweet.\n",
    "* Traitez le tweet en utilisant la fonction `process_tweet()` importée et enregistrez la liste des mots du tweet.\n",
    "* Parcourez chaque mot de la liste des mots traités\n",
    "    * Pour chaque mot, vérifiez le dictionnaire `freqs` pour le décompte lorsque ce mot a une étiquette positive '1'. (Vérifiez la clé (mot, 1.0))\n",
    "    * Faites de même pour le décompte lorsque le mot est associé à l'étiquette négative '0'. (Vérifiez la clé (mot, 0.0).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    Aide\n",
    "</summary>\n",
    "\n",
    "* Assurez-vous de gérer les cas où la clé (mot, étiquette) n'est pas trouvée dans le dictionnaire.</li>\n",
    "* Cherchez comment utiliser la méthode `.get()` d'un dictionnaire Python sur le web. Voici un <a href=\"https://www.programiz.com/python-programming/methods/dictionary/get\" > exemple </a>.</li>\n",
    "</details>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tweet, freqs):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet: a list of words for one tweet\n",
    "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
    "    Output: \n",
    "        x: a feature vector of dimension (1,3)\n",
    "    '''\n",
    "    # process_tweet tokenise, stem, et enlève les stopwords\n",
    "    word_l = process_tweet(tweet)\n",
    "    \n",
    "    # features à 0\n",
    "    x = np.zeros((1, 3)) \n",
    "    \n",
    "    #biais fixé à 1\n",
    "    x[0,0] = 1 \n",
    "    \n",
    "    ### Code ici - Début\n",
    "    \n",
    "    # Boucle sur chacun des mots\n",
    "    for word in word_l:\n",
    "        \n",
    "        # Incrémente le score positif du mot avec sa fréquence dans les tweets positifs\n",
    "        x[0,1] += freqs.get((word,1),0.0)\n",
    "        \n",
    "        # Incrémente le score négatif du mot avec sa fréquence dans les tweets négatifs\n",
    "        x[0,2] += freqs.get((word,0),0.0)\n",
    "        \n",
    "    ### Code ici - Fin\n",
    "    assert(x.shape == (1, 3))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "[[1.00e+00 3.02e+03 6.10e+01]]\n"
     ]
    }
   ],
   "source": [
    "# Testez votre fonction\n",
    "\n",
    "# test 1:\n",
    "# test sur une donnée d'entrainement\n",
    "tmp1 = extract_features(train_x[0], freqs)\n",
    "print(train_x[0])\n",
    "print(tmp1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected output\n",
    "```\n",
    "[[1.00e+00 3.02e+03 6.10e+01]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# test 2:\n",
    "# Vérifiez dans le cas où les mots ne sont pas dans le dictionnaire des frequences\n",
    "tmp2 = extract_features('blorb bleeeeb bloooob', freqs)\n",
    "print(tmp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected output\n",
    "```\n",
    "[[1. 0. 0.]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 3: Entrainement du Modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost after training is 0.12778644.\n",
      "The resulting vector of weights is [2.9e-07, 0.00111999, -0.00099142]\n"
     ]
    }
   ],
   "source": [
    "# collecter les features 'x' dans une matrice 'X'\n",
    "X = np.zeros((len(train_x), 3))\n",
    "for i in range(len(train_x)):\n",
    "    X[i, :]= extract_features(train_x[i], freqs)\n",
    "\n",
    "# Etiquettes correspondant à X\n",
    "Y = train_y\n",
    "\n",
    "# Applique la descente de gradient\n",
    "J, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 7000)\n",
    "print(f\"The cost after training is {J:.8f}.\")\n",
    "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 4: Testez votre regression logistique\n",
    "\n",
    "Testez votre regression logistique sur de nouvelles données encore jamais vues par le modèle.\n",
    "\n",
    "**Instructions** : Complétez la fonction `predict_tweet`\n",
    "qui prédit si un tweet est positif ou négatif.\n",
    "\n",
    "* Extraire les features du tweet\n",
    "* Appliquez les poids du modèle appris pour calculer les combinaisons linéaires z\n",
    "* Appliquez la fonction sigmoid aux combianaisons linéaires pour prédire la valeur entre 0 et 1.\n",
    "\n",
    "$$y_{pred} = sigmoid(\\mathbf{x} \\cdot \\theta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tweet(tweet, freqs, theta):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet: a string\n",
    "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
    "        theta: (3,1) vector of weights\n",
    "    Output: \n",
    "        y_pred: the probability of a tweet being positive or negative\n",
    "    '''\n",
    "    ### Code ici - Début\n",
    "    \n",
    "    # Extraire les features du tweet\n",
    "    x = extract_features(tweet, freqs)\n",
    "    \n",
    "    # Calculez la prédiction\n",
    "    y_pred = sigmoid(x.dot(theta))\n",
    "    \n",
    "    ### Code ici - Fin\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am happy -> 0.540529\n",
      "I am bad -> 0.490537\n",
      "this movie should have been great. -> 0.534294\n",
      "great -> 0.533751\n",
      "great great -> 0.567195\n",
      "great great great -> 0.600038\n",
      "great great great great -> 0.632006\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to test your function\n",
    "for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n",
    "    print( '%s -> %f' % (tweet, predict_tweet(tweet, freqs, theta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.96047076]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vérifiez avec vos propres tweets\n",
    "my_tweet = 'I am learning :)'\n",
    "predict_tweet(my_tweet, freqs, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vérifiez la performance sur l'ensemble de test\n",
    "\n",
    "** Instructions:** Implementer `test_logistic_regression` \n",
    "* À partir des données de test et des poids de votre modèle entraîné, calculez la précision (accuracy) de votre modèle de régression logistique.\n",
    "* Utilisez votre fonction `predict_tweet()` pour faire des prédictions sur chaque tweet de l'ensemble de test.\n",
    "* Si la prédiction est > 0,5, définissez la classification du modèle `y_hat` à 1, sinon définissez la classification du modèle `y_hat` à 0.\n",
    "* Une prédiction est exacte lorsque `y_hat` est égal à `test_y`. Additionnez toutes les instances où elles sont égales et divisez par `m`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Use np.asarray() to convert a list to a numpy array</li>\n",
    "    <li>Use np.squeeze() to make an (m,1) dimensional array into an (m,) array </li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_logistic_regression(test_x, test_y, freqs, theta):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        test_x: a list of tweets\n",
    "        test_y: (m, 1) vector with the corresponding labels for the list of tweets\n",
    "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
    "        theta: weight vector of dimension (3, 1)\n",
    "    Output: \n",
    "        accuracy: (# of tweets classified correctly) / (total # of tweets)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Code ici - Début\n",
    "    \n",
    "    # Liste pour stocker les predictions\n",
    "    y_hat = []\n",
    "    \n",
    "    for tweet in test_x:\n",
    "        # Obtenir la prédiction pour le tweet\n",
    "        y_pred = predict_tweet(tweet, freqs, theta)\n",
    "        \n",
    "        if y_pred > 0.5:\n",
    "            # Ajouter 1.0 à la fin de la liste\n",
    "            y_hat.append(float(1))\n",
    "        else:\n",
    "            # Ajouter 0 à la fin de la list\n",
    "            y_hat.append(float(0))\n",
    "    ### Code ici - Fin\n",
    "\n",
    "    # With the above implementation, y_hat is a list, but test_y is (m,1) array\n",
    "    # convert both to one-dimensional arrays in order to compare them using the '==' operator\n",
    "    accuracy = float(1/len(test_y))*np.sum(np.squeeze(np.asarray(y_hat))==np.squeeze(test_y))\n",
    "\n",
    "    \n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression model's accuracy = 0.9950\n"
     ]
    }
   ],
   "source": [
    "tmp_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\n",
    "print(f\"Logistic regression model's accuracy = {tmp_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output: \n",
    "```0.9950```  \n",
    "Pretty good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 5: Analyse des erreurs\n",
    "\n",
    "Dans cette partie, vous verrez quelques tweets que votre modèle a mal classés. Pourquoi pensez-vous que les mauvaises classifications se sont produites ? Plus précisément, quel type de tweets votre modèle classe-t-il incorrectement ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Predicted Tweet\n",
      "THE TWEET IS: I'm playing Brain Dots : ) #BrainDots\n",
      "http://t.co/UGQzOx0huu\n",
      "THE PROCESSED TWEET IS: [\"i'm\", 'play', 'brain', 'dot', 'braindot']\n",
      "1\t0.48026257\tb\"i'm play brain dot braindot\"\n",
      "THE TWEET IS: I'm playing Brain Dots : ) #BrainDots http://t.co/aOKldo3GMj http://t.co/xWCM9qyRG5\n",
      "THE PROCESSED TWEET IS: [\"i'm\", 'play', 'brain', 'dot', 'braindot']\n",
      "1\t0.48026257\tb\"i'm play brain dot braindot\"\n",
      "THE TWEET IS: I'm playing Brain Dots : ) #BrainDots http://t.co/R2JBO8iNww http://t.co/ow5BBwdEMY\n",
      "THE PROCESSED TWEET IS: [\"i'm\", 'play', 'brain', 'dot', 'braindot']\n",
      "1\t0.48026257\tb\"i'm play brain dot braindot\"\n",
      "THE TWEET IS: @msarosh Uff Itna Miss karhy thy ap :p\n",
      "THE PROCESSED TWEET IS: ['uff', 'itna', 'miss', 'karhi', 'thi', 'ap', ':p']\n",
      "1\t0.47367529\tb'uff itna miss karhi thi ap :p'\n",
      "THE TWEET IS: @phenomyoutube u probs had more fun with david than me : (\n",
      "THE PROCESSED TWEET IS: ['u', 'prob', 'fun', 'david']\n",
      "0\t0.50898403\tb'u prob fun david'\n",
      "THE TWEET IS: pats jay : (\n",
      "THE PROCESSED TWEET IS: ['pat', 'jay']\n",
      "0\t0.50084007\tb'pat jay'\n",
      "THE TWEET IS: @bae_ts WHATEVER STIL L YOUNG &gt;:-(\n",
      "THE PROCESSED TWEET IS: ['whatev', 'stil', 'l', 'young', '>:-(']\n",
      "0\t0.50025721\tb'whatev stil l young >:-('\n",
      "THE TWEET IS: my beloved grandmother : ( https://t.co/wt4oXq5xCf\n",
      "THE PROCESSED TWEET IS: ['belov', 'grandmoth']\n",
      "0\t0.50000007\tb'belov grandmoth'\n",
      "THE TWEET IS: @CHEDA_KHAN Thats life. I get calls from people I havent seen in 20 years and its always favours : (\n",
      "THE PROCESSED TWEET IS: ['that', 'life', 'get', 'call', 'peopl', 'havent', 'seen', '20', 'year', 'alway', 'favour']\n",
      "0\t0.50291498\tb'that life get call peopl havent seen 20 year alway favour'\n",
      "THE TWEET IS: @ITVCentral #Midlands Yes thanks for the depressing weather forecast, where the word 'rain' was mentioned several times :-(\n",
      "THE PROCESSED TWEET IS: ['midland', 'ye', 'thank', 'depress', 'weather', 'forecast', 'word', 'rain', 'mention', 'sever', 'time', ':-(']\n",
      "0\t0.52514574\tb'midland ye thank depress weather forecast word rain mention sever time :-('\n"
     ]
    }
   ],
   "source": [
    "print('Label Predicted Tweet')\n",
    "for x,y in zip(test_x,test_y):\n",
    "    y_hat = predict_tweet(x, freqs, theta)\n",
    "\n",
    "    if np.abs(y - (y_hat > 0.5)) > 0:\n",
    "        print('THE TWEET IS:', x)\n",
    "        print('THE PROCESSED TWEET IS:', process_tweet(x))\n",
    "        print('%d\\t%0.8f\\t%s' % (y, y_hat, ' '.join(process_tweet(x)).encode('ascii', 'ignore')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later in this specialization, we will see how we can use deep learning to improve the prediction performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 6: Prediction avec votre propre tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['absolut', 'love', 'movi', 'entertain', 'great', 'cast']\n",
      "[[0.59579304]]\n",
      "Positive sentiment\n"
     ]
    }
   ],
   "source": [
    "# Changez le tweet pour tester\n",
    "my_tweet = 'Absolutely loved the movie! It was so entertaining and had a great cast.'\n",
    "\n",
    "print(process_tweet(my_tweet))\n",
    "y_hat = predict_tweet(my_tweet, freqs, theta)\n",
    "print(y_hat)\n",
    "if y_hat > 0.5:\n",
    "    print('Positive sentiment')\n",
    "else: \n",
    "    print('Negative sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "NLPC1-1"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
