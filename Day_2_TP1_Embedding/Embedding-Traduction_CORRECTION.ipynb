{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding - Traduction machine naive\n",
    "\n",
    "Vous allez maintenant mettre en place votre premier système de traduction automatique!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import pickle\n",
    "import string\n",
    "\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn\n",
    "\n",
    "from utils import get_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "\n",
    "# 1. Embeddings pour mots en Anglais et en Français"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Le sous-emsemble de données\n",
    "\n",
    "Pour réaliser cet exercice, nous utiliserons une sous-ensemble de word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_embeddings_subset = pickle.load(open(\"en_embeddings.p\", \"rb\"))\n",
    "fr_embeddings_subset = pickle.load(open(\"fr_embeddings.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consulter les données\n",
    "\n",
    "* en_embeddings_subset: la clé (key) est un mot en anglais et la valeur est un\n",
    "tableau de 300 dimensions, qui est l'embedding de ce mot.\n",
    "```\n",
    "'the': array([ 0.08007812,  0.10498047,  0.04980469,  0.0534668 , -0.06738281, ....\n",
    "```\n",
    "\n",
    "* fr_embeddings_subset: tla clé (key) est un mot en français et la valeur est un\n",
    "tableau de 300 dimensions, qui est l'embedding de ce mot.\n",
    "```\n",
    "'la': array([-6.18250e-03, -9.43867e-04, -8.82648e-03,  3.24623e-02,...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instruction**: Consultez les premiers éléments de `en_embeddings_subset` et `fr_embeddings_subset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "[ 0.08007812  0.10498047  0.04980469  0.0534668  -0.06738281]\n",
      "la\n",
      "[-0.0061825  -0.00094387 -0.00882648  0.0324623  -0.0218281 ]\n"
     ]
    }
   ],
   "source": [
    "### Code ici\n",
    "#print(list(en_embeddings_subset.items())[0])\n",
    "print(list(en_embeddings_subset.keys())[0])\n",
    "print(list(en_embeddings_subset.values())[0][:5])\n",
    "#print(list(fr_embeddings_subset.items())[0])\n",
    "print(list(fr_embeddings_subset.keys())[0])\n",
    "print(list(fr_embeddings_subset.values())[0][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Charger les deux dictionnaires qui font correspondre les mots anglais aux mots français :\n",
    "* Un dictionnaire d'entraînement\n",
    "* Un dictionnaire de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/azizamerzouki/Library/CloudStorage/GoogleDrive-aziza.merzouki1701@gmail.com/Mon Drive/UNIGE_Formation_Continues_DSA/AM-DSA-Course/Practicals_CORRECTION/Day 2/Day 2 - Embeddings/utils.py:55: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  en = my_file.loc[i][0]\n",
      "/Users/azizamerzouki/Library/CloudStorage/GoogleDrive-aziza.merzouki1701@gmail.com/Mon Drive/UNIGE_Formation_Continues_DSA/AM-DSA-Course/Practicals_CORRECTION/Day 2/Day 2 - Embeddings/utils.py:56: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  fr = my_file.loc[i][1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the English to French training dictionary is 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/azizamerzouki/Library/CloudStorage/GoogleDrive-aziza.merzouki1701@gmail.com/Mon Drive/UNIGE_Formation_Continues_DSA/AM-DSA-Course/Practicals_CORRECTION/Day 2/Day 2 - Embeddings/utils.py:55: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  en = my_file.loc[i][0]\n",
      "/Users/azizamerzouki/Library/CloudStorage/GoogleDrive-aziza.merzouki1701@gmail.com/Mon Drive/UNIGE_Formation_Continues_DSA/AM-DSA-Course/Practicals_CORRECTION/Day 2/Day 2 - Embeddings/utils.py:56: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  fr = my_file.loc[i][1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the English to French test dictionary is 5000\n"
     ]
    }
   ],
   "source": [
    "# Chargement du dictionnaire Anglais -> Français\n",
    "en_fr_train = get_dict('en-fr.train.txt')\n",
    "print('The length of the English to French training dictionary is', len(en_fr_train))\n",
    "en_fr_test = get_dict('en-fr.test.txt')\n",
    "print('The length of the English to French test dictionary is', len(en_fr_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consulter le dictionnaire Anglais -> Français\n",
    "\n",
    "* `en_fr_train` est un dictionnaire où la clé est le mot anglais et la valeur\n",
    "est la traduction française de ce mot anglais.\n",
    "```\n",
    "{'the': 'la',\n",
    " 'and': 'et',\n",
    " 'was': 'était',\n",
    " 'for': 'pour',\n",
    "```\n",
    "\n",
    "* `en_fr_test` est similaire à `en_fr_train`, mais pour l'étape de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 'la', 'and': 'et', 'was': 'était', 'for': 'pour'}\n"
     ]
    }
   ],
   "source": [
    "### Code ici\n",
    "print(dict(list(en_fr_train.items())[0:4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1-1\"></a>\n",
    "\n",
    "## 1.1 Embeddings et matrice de transformation\n",
    "\n",
    "<a name=\"ex-01\"></a>\n",
    "#### Traduction du dictionnaire anglais -> français en utilisant des embeddings\n",
    "\n",
    "Vous allez maintenant implémenter une fonction `get_matrices`, qui prend les données chargées\n",
    "et renvoie les matrices `X` et `Y`.\n",
    "\n",
    "Entrées :\n",
    "- `en_fr` : Dictionnaire anglais -> français\n",
    "- `en_embeddings` : Dictionnaire mots anglais vers les embeddings\n",
    "- `fr_embeddings` : Dictionnaire mots français vers les embeddings\n",
    "\n",
    "Renvoie :\n",
    "- La matrice `X` et la matrice `Y`, où chaque ligne dans X est l'embedding de\n",
    "  d'un mot anglais, et la même ligne dans Y est l'embedding de la version française\n",
    "  de ce mot anglais.\n",
    "\n",
    "<div style=\"width:image width px; font-size:100%; text-align:center;\">\n",
    "<img src='X_to_Y.jpg' alt=\"texte alternatif\" width=\"largeur\" height=\"hauteur\" style=\"largeur:800px;hauteur:200px;\" /> Figure 2 </div>\n",
    "\n",
    "Utilisez le dictionnaire `en_fr` pour vous assurer que la i-ème ligne de la matrice `X`\n",
    "correspond à la i-ème ligne de la matrice `Y`.\n",
    "\n",
    "<a name=\"1-1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions**: Completez la function `get_matrices()`:\n",
    "* Récupérez les embeddings des mots français et anglais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Aide</b></font>\n",
    "</summary>\n",
    "    <p>\n",
    "        <ul>\n",
    "            <li>Utilisez les dictionnaires `french_vecs` et `english_vecs`</li>\n",
    "        </ul>\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrices(en_fr, french_vecs, english_vecs):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        en_fr: English to French dictionary\n",
    "        french_vecs: French words to their corresponding word embeddings.\n",
    "        english_vecs: English words to their corresponding word embeddings.\n",
    "    Output: \n",
    "        X: a matrix where the columns are the English embeddings.\n",
    "        Y: a matrix where the columns correspong to the French embeddings.\n",
    "        R: the projection matrix that minimizes the F norm ||X R -Y||^2.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # X_l et Y_l sont les listes d'embedding Anglais et Français\n",
    "    X_l = list()\n",
    "    Y_l = list()\n",
    "\n",
    "    # Stocker les mots Anglais du dictionnaire des embeddings Anglais\n",
    "    english_set = set(english_vecs.keys())\n",
    "\n",
    "    # Stocker les mots français du dictionnaire des embeddings Français\n",
    "    french_set = set(french_vecs.keys())\n",
    "\n",
    "    # Stocker les mots français du dictionnaire Anglais-Français\n",
    "    french_words = set(en_fr.values())\n",
    "\n",
    "    # Boucle sur toutes les paires de mots anglais et français dans le dictionnaire anglais-français.\n",
    "\n",
    "    for en_word, fr_word in en_fr.items():\n",
    "\n",
    "        # Vérifie que le mot français a un embedding et que le mot anglais a un embedding.\n",
    "        if fr_word in french_set and en_word in english_set:\n",
    "\n",
    "            ### Code ici - Début\n",
    "            # Recuperez l'embedding du mot anglais\n",
    "            en_vec = english_vecs[en_word]\n",
    "\n",
    "            # Recuperez l'embedding du mot français\n",
    "            fr_vec = french_vecs[fr_word]\n",
    "            ### Code ici - Fin\n",
    "\n",
    "            # Ajoute l'embedding anglais à la liste\n",
    "            X_l.append(en_vec)\n",
    "\n",
    "            # Ajoute l'embedding français à la liste\n",
    "            Y_l.append(fr_vec)\n",
    "\n",
    "    # Empile les vecteurs X_l dans une matrice X\n",
    "    X = np.vstack(X_l)\n",
    "\n",
    "    # Empile les vecteurs Y_l dans une matrice Y\n",
    "    Y = np.vstack(Y_l)\n",
    "\n",
    "    return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupére l'ensemble d'entrainement\n",
    "X_train, Y_train = get_matrices(\n",
    "    en_fr_train, fr_embeddings_subset, en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "\n",
    "# 2. Traduction\n",
    "\n",
    "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='e_to_f.jpg' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:700px;height:200px;\" /> Figure 1 </div>\n",
    "\n",
    "<a name=\"2-1\"></a>\n",
    "## 2.1 Traduction comme transformation linéaire des embeddings\n",
    "\n",
    "Étant donné des dictionnaires d'embedding de mots anglais et français, vous créerez une matrice de transformation `R`.\n",
    "* Étant donné un embedding de mot anglais, $\\mathbf{e}$, vous pouvez le multiplier par $\\mathbf{eR}$ pour obtenir un nouvel embedding de mot $\\mathbf{f}$.\n",
    "    * À la fois $\\mathbf{e}$ et $\\mathbf{f}$ sont des [vecteurs ligne](https://en.wikipedia.org/wiki/Row_and_column_vectors).\n",
    "* Vous pouvez ensuite calculer les voisins les plus proches de `f` dans les embeddings français et recommander le mot le plus similaire à l'embedding de mot transformé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trouver une matrice `R` qui minimise l'équation suivante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction de coût \n",
    "\n",
    "$$ \\frac{1}{m} \\|  \\mathbf{X R} - \\mathbf{Y} \\|_{F}^{2}$$\n",
    "\n",
    "où $m$ est le nombre d'exemples (nombre de lignes dans $\\mathbf{X}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex-02\"></a>\n",
    "\n",
    "#### Implémentation du mécanisme de traduction.\n",
    "\n",
    "#### Étape 1 : Calcul du coût\n",
    "* La fonction de coût :\n",
    "$$ L(X, Y, R)=\\frac{1}{m}\\sum_{i=1}^{m} \\sum_{j=1}^{n}\\left( a_{i j} \\right)^{2}$$\n",
    "\n",
    "où $a_{i j}$ est la valeur dans la $i$-ème ligne et la $j$-ème colonne de la matrice $\\mathbf{XR}-\\mathbf{Y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(X, Y, R):\n",
    "    '''\n",
    "    Inputs: \n",
    "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
    "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
    "        R: a matrix of dimension (n,n) - transformation matrix from English to French vector space embeddings.\n",
    "    Outputs:\n",
    "        L: a matrix of dimension (m,n) - the value of the loss function for given X, Y and R.\n",
    "    '''\n",
    "    # m is the number of rows in X\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # diff is XR - Y\n",
    "    diff = np.dot(X,R)-Y\n",
    "\n",
    "    # diff_squared is the element-wise square of the difference\n",
    "    diff_squared = np.square(diff)\n",
    "\n",
    "    # sum_diff_squared is the sum of the squared elements\n",
    "    sum_diff_squared = np.sum(diff_squared)\n",
    "\n",
    "    # loss i the sum_diff_squard divided by the number of examples (m)\n",
    "    loss = sum_diff_squared/m\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 2 : Calcul du gradient du coût par rapport à la matrice de transformation R\n",
    "\n",
    "* Calculer le gradient de la perte par rapport à la matrice de transformation `R`.\n",
    "* Le gradient est une matrice qui indique dans quelle mesure un petit changement dans `R` affecte le changement de la fonction de perte.\n",
    "* Le gradient nous donne la direction dans laquelle nous devons diminuer `R` pour minimiser la fonction de coût.\n",
    "* $m$ est le nombre d'exemples d'entraînement (nombre de lignes dans $X$).\n",
    "* La formule du gradient de la fonction de perte $𝐿(𝑋,𝑌,𝑅)$ est la suivante :\n",
    "\n",
    "$$\\frac{d}{dR}𝐿(𝑋,𝑌,𝑅)=\\frac{d}{dR}\\Big(\\frac{1}{m}\\| X R -Y\\|_{F}^{2}\\Big) = \\frac{2}{m}X^{T} (X R - Y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, Y, R):\n",
    "    '''\n",
    "    Inputs: \n",
    "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
    "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
    "        R: a matrix of dimension (n,n) - transformation matrix from English to French vector space embeddings.\n",
    "    Outputs:\n",
    "        g: a matrix of dimension (n,n) - gradient of the loss function L for given X, Y and R.\n",
    "    '''\n",
    "    # m is the number of rows in X\n",
    "    m = X.shape[0]\n",
    "\n",
    "    # gradient is X^T(XR - Y) * 2/m\n",
    "    gradient = np.dot(X.T,np.dot(X,R)-Y)*2/m\n",
    "    return gradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 3 : Recherche de la meilleure matrice R avec l'algorithme de descente de gradient\n",
    "\n",
    "#### Descente de gradient\n",
    "\n",
    "La [descente de gradient](https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html) est un algorithme itératif utilisé pour rechercher l'optimum d'une fonction. \n",
    "* Comme mentionné précédemment, le gradient de la perte par rapport à la matrice indique dans quelle mesure un petit changement dans une coordonnée de cette matrice affecte le changement de la fonction de perte.\n",
    "* La descente de gradient utilise cette information pour changer de manière itérative la matrice `R` jusqu'à ce que nous atteignions un point où la perte est minimisée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entraînement avec un nombre fixe d'itérations\n",
    "\n",
    "La plupart du temps, nous itérons pour un nombre fixe d'étapes d'entraînement plutôt que d'itérer jusqu'à ce que la perte descende en dessous d'un seuil."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudo-code :\n",
    "1. Calculer le gradient $g$ de la perte par rapport à la matrice $R$.\n",
    "2. Mettre à jour $R$ avec la formule :\n",
    "$$R_{\\text{new}}= R_{\\text{old}}-\\alpha g$$\n",
    "\n",
    "Où $\\alpha$ est le taux d'apprentissage, qui est un scalaire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Le taux d'apprentissage ou \"pas\" $\\alpha$ est un coefficient qui décide combien nous voulons changer $R$ à chaque étape.\n",
    "* Si nous changeons $R$ trop rapidement, nous pourrions passer à côté de l'optimum en prenant un pas trop grand.\n",
    "* Si nous apportons seulement de petits changements à $R$, nous aurons besoin de nombreuses étapes pour atteindre l'optimum.\n",
    "* Le taux d'apprentissage $\\alpha$ est utilisé pour contrôler ces changements.\n",
    "* Les valeurs de $\\alpha$ sont choisies en fonction du problème, et nous utiliserons `learning_rate`$=0.0003$ comme valeur par défaut pour notre algorithme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructions: Implementez `align_embeddings()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Aide</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Utilisez la fonction 'compute_gradient()' pour obtenir le gradient à chaque itération</li>\n",
    "\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_embeddings(X, Y, train_steps=100, learning_rate=0.0003):\n",
    "    '''\n",
    "    Inputs:\n",
    "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
    "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
    "        train_steps: positive int - describes how many steps will gradient descent algorithm do.\n",
    "        learning_rate: positive float - describes how big steps will  gradient descent algorithm do.\n",
    "    Outputs:\n",
    "        R: a matrix of dimension (n,n) - the projection matrix that minimizes the F norm ||X R -Y||^2\n",
    "    '''\n",
    "    np.random.seed(129)\n",
    "\n",
    "    # Nombre de lignes et de colonnes de R est égal au nombre de dimensions de l'embedding (e.g. 300)\n",
    "    R = np.random.rand(X.shape[1], X.shape[1])\n",
    "\n",
    "    for i in range(train_steps):\n",
    "        if i % 25 == 0:\n",
    "            print(f\"loss at iteration {i} is: {compute_loss(X, Y, R):.4f}\")\n",
    "        ### Code ici - Début\n",
    "        # Utilisez la fonction implémentée plus haut et qui permet de calculer le gradient\n",
    "        gradient = compute_gradient(X, Y, R)\n",
    "\n",
    "        # Mettre à jour R en soustrayant le taux d'apprentissage fois le gradient\n",
    "        R -= learning_rate*gradient\n",
    "        ### Code ici - Fin\n",
    "    return R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iteration 0 is: 3.7242\n",
      "loss at iteration 25 is: 3.6283\n",
      "loss at iteration 50 is: 3.5350\n",
      "loss at iteration 75 is: 3.4442\n"
     ]
    }
   ],
   "source": [
    "# Tester l'implémentation\n",
    "np.random.seed(129)\n",
    "m = 10\n",
    "n = 5\n",
    "X = np.random.rand(m, n)\n",
    "Y = np.random.rand(m, n) * .1\n",
    "R = align_embeddings(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```\n",
    "loss at iteration 0 is: 3.7242\n",
    "loss at iteration 25 is: 3.6283\n",
    "loss at iteration 50 is: 3.5350\n",
    "loss at iteration 75 is: 3.4442\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculer la matrice de transformation R\n",
    "\n",
    "Calculer la matrice de transformation R, en utilisant l'ensemble d'entraînement et en appelant la fonction align_embeddings().\n",
    "\n",
    "REMARQUE : La cellule de code ci-dessous prendra quelques minutes pour s'exécuter complètement (~3 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iteration 0 is: 963.0146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iteration 25 is: 97.8292\n",
      "loss at iteration 50 is: 26.8329\n",
      "loss at iteration 75 is: 9.7893\n",
      "loss at iteration 100 is: 4.3776\n",
      "loss at iteration 125 is: 2.3281\n",
      "loss at iteration 150 is: 1.4480\n",
      "loss at iteration 175 is: 1.0338\n",
      "loss at iteration 200 is: 0.8251\n",
      "loss at iteration 225 is: 0.7145\n",
      "loss at iteration 250 is: 0.6534\n",
      "loss at iteration 275 is: 0.6185\n",
      "loss at iteration 300 is: 0.5981\n",
      "loss at iteration 325 is: 0.5858\n",
      "loss at iteration 350 is: 0.5782\n",
      "loss at iteration 375 is: 0.5735\n"
     ]
    }
   ],
   "source": [
    "R_train = align_embeddings(X_train, Y_train, train_steps=400, learning_rate=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Expected Output\n",
    "\n",
    "```\n",
    "loss at iteration 0 is: 963.0146\n",
    "loss at iteration 25 is: 97.8292\n",
    "loss at iteration 50 is: 26.8329\n",
    "loss at iteration 75 is: 9.7893\n",
    "loss at iteration 100 is: 4.3776\n",
    "loss at iteration 125 is: 2.3281\n",
    "loss at iteration 150 is: 1.4480\n",
    "loss at iteration 175 is: 1.0338\n",
    "loss at iteration 200 is: 0.8251\n",
    "loss at iteration 225 is: 0.7145\n",
    "loss at iteration 250 is: 0.6534\n",
    "loss at iteration 275 is: 0.6185\n",
    "loss at iteration 300 is: 0.5981\n",
    "loss at iteration 325 is: 0.5858\n",
    "loss at iteration 350 is: 0.5782\n",
    "loss at iteration 375 is: 0.5735\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Tester la traduction\n",
    "\n",
    "### Algorithme des k-plus proches voisins (k-NN)\n",
    "\n",
    "[L'algorithme des k-plus proches voisins](https://fr.wikipedia.org/wiki/K-plus_proches_voisins)\n",
    "* L'algorithme k-NN est une méthode qui prend un vecteur en entrée et trouve les autres vecteurs du jeu de données qui lui sont les plus proches.\n",
    "* Le \"k\" représente le nombre de \"plus proches voisins\" à trouver (par exemple, k=2 trouve les deux voisins les plus proches).\n",
    "\n",
    "### Recherche de l'embedding de la traduction\n",
    "Puisque nous approximons la fonction de traduction des embeddings anglais vers les embeddings français par une matrice de transformation linéaire $\\mathbf{R}$, la plupart du temps, nous n'obtiendrons pas l'embedding exacte d'un mot français lorsque nous transformons l'embedding $\\mathbf{e}$ d'un mot anglais dans l'espace d'embedding français.\n",
    "* C'est là que k-NN devient utile ! En utilisant le 1-NN avec $\\mathbf{eR}$ comme entrée, nous pouvons rechercher un embedding $\\mathbf{f}$ (sous forme de ligne) dans la matrice $\\mathbf{Y}$ qui est la plus proche du vecteur transformé $\\mathbf{eR}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarité cosinus\n",
    "La similarité cosinus entre les vecteurs $u$ et $v$ est calculée comme le cosinus de l'angle entre eux.\n",
    "La formule est la suivante :\n",
    "\n",
    "$$\\cos(u,v) = \\frac{u \\cdot v}{\\left\\|u\\right\\| \\left\\|v\\right\\|}$$\n",
    "* $\\cos(u,v)$ = $1$ lorsque $u$ et $v$ se trouvent sur la même ligne et ont la même direction.\n",
    "* $\\cos(u,v)$ est $-1$ lorsqu'ils ont des directions exactement opposées.\n",
    "* $\\cos(u,v)$ est $0$ lorsque les vecteurs sont orthogonaux (perpendiculaires) les uns aux autres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbor(v, candidates, k=1):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      - v, the vector you are going find the nearest neighbor for\n",
    "      - candidates: a set of vectors where we will find the neighbors\n",
    "      - k: top k nearest neighbors to find\n",
    "    Output:\n",
    "      - k_idx: the indices of the top k closest vectors in sorted form\n",
    "    \"\"\"\n",
    "    similarity_l = []\n",
    "\n",
    "    # for each candidate vector...\n",
    "    for row in candidates:\n",
    "        # get the cosine similarity\n",
    "        cos_similarity = np.dot(v,row)/(np.linalg.norm(v)*np.linalg.norm(row))\n",
    "\n",
    "        # append the similarity to the list\n",
    "        similarity_l.append(cos_similarity)\n",
    "     \n",
    "    # sort the similarity list and get the indices of the sorted list\n",
    "    sorted_ids = np.argsort(similarity_l)\n",
    "    \n",
    "    # get the indices of the k most similar candidate vectors\n",
    "    k_idx = sorted_ids[-k:]\n",
    "    \n",
    "    return k_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9 9 9]\n",
      " [1 0 5]\n",
      " [2 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# Testez l'implementation du k-nn\n",
    "v = np.array([1, 0, 1])\n",
    "candidates = np.array([[1, 0, 5], [-2, 5, 3], [2, 0, 1], [6, -9, 5], [9, 9, 9]])\n",
    "print(candidates[nearest_neighbor(v, candidates, 3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "`[[9 9 9]\n",
    " [1 0 5]\n",
    " [2 0 1]]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testez votre traduction et calculez sa précision\n",
    "\n",
    "* Parcourez les embeddings de mots en anglais transformés et vérifiez si le vecteur de mot français le plus proche appartient au mot français qui est la traduction réelle.\n",
    "* Obtenez un indice de l'embedding français le plus proche en utilisant `nearest_neighbor` (avec l'argument `k=1`), et comparez-le à l'indice de l'embedding en anglais que vous venez de transformer.\n",
    "* Gardez une trace du nombre de fois où vous obtenez la traduction correcte.\n",
    "* Calculez la précision comme suit : $$\\text{précision} = \\frac{\\#(\\text{prédictions correctes})}{\\#(\\text{prédictions totales})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vocabulary(X, Y, R):\n",
    "    '''\n",
    "    Input:\n",
    "        X: a matrix where the columns are the English embeddings.\n",
    "        Y: a matrix where the columns correspong to the French embeddings.\n",
    "        R: the transform matrix which translates word embeddings from\n",
    "        English to French word vector space.\n",
    "    Output:\n",
    "        accuracy: for the English to French capitals\n",
    "    '''\n",
    "\n",
    "    # The prediction is X times R\n",
    "    pred = np.dot(X,R)\n",
    "\n",
    "    # initialize the number correct to zero\n",
    "    num_correct = 0\n",
    "\n",
    "    # loop through each row in pred (each transformed embedding)\n",
    "    for i in range(len(pred)):\n",
    "        # get the index of the nearest neighbor of pred at row 'i'; also pass in the candidates in Y\n",
    "        pred_idx = nearest_neighbor(pred[i], Y, k=1)\n",
    "\n",
    "        # if the index of the nearest neighbor equals the row of i... \\\n",
    "        if pred_idx == i:\n",
    "            # increment the number correct by 1.\n",
    "            num_correct += 1\n",
    "\n",
    "    # accuracy is the number correct divided by the number of rows in 'pred' (also number of rows in X)\n",
    "    accuracy = num_correct/pred.shape[0]\n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelle performance sur les données de test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, Y_val = get_matrices(en_fr_test, fr_embeddings_subset, en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test set is 0.557\n"
     ]
    }
   ],
   "source": [
    "acc = test_vocabulary(X_val, Y_val, R_train)  # this might take a minute or two\n",
    "print(f\"accuracy on test set is {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "```\n",
    "0.557\n",
    "```\n",
    "### Résumé\n",
    "Vous avez réussi à traduire des mots d'une langue à une autre sans les avoir jamais vus avec une précision de presque 56% en utilisant de l'algèbre linéaire de base et en apprenant une correspondance entre les mots d'une langue à une autre grâce aux embeddings!"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "NLPC1-4"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
